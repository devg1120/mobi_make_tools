
 <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
  <head>
    <meta content="http://www.w3.org/1999/xhtml; charset=utf-8" http-equiv="Cont
ent-Type" />
    <title>Measure before Optimizing</title>
    <meta name="description" content="undefined" />
    <meta name="author" content="undefined" />
    <link rel="stylesheet" type="text/css" href="/home/gusa1120/es2015/cheerio-mobi-ts9/undefined" />
  </head>
   <body>
<div class="sect1" lang="en"><div class="titlepage"><div><h2 class="title" style="clear: both"><a id="id2960884">Measure before Optimizing</a></h2></div></div><p>When you have real-world evidence that your application is too
slow, then (and <span class="emphasis"><em>only</em></span> then) is the time to think
about optimizing the code.  But before you do more than think about
optimizing, measure.</p><p>Recall Rob Pike&apos;s six rules in <a href="philosophychapter.html" title="Chapter&#xFFFD;1.&#xFFFD;Philosophy">Chapter&#xFFFD;1</a>.  One of the lessons that the original
Unix programmers learned early is that intuition is a poor guide to
where the bottlenecks are, even for one who knows the code in question
intimately. Unixes, unlike most other operating systems, usually come with
profilers; use them.</p><p>Reading profiler results is something of an art.  There are a
couple of recurring problems: one is instrumentation noise, another is
the effect of imposed external latencies, and a third is
overweighting of upper nodes in the call graph.</p><p>The instrumentation-noise problem is fundamental.  Profilers
work by inserting instructions that report execution time at the entry
and exit points of subroutines, also at fixed intervals within the inline
code of routines.  These instructions themselves take time to execute.
The effect is to reduce the dispersion of call times: very short
subroutines tend to look more expensive than they are, with a 
lot of noise in their comparative call times, while for longer
ones the instrumentation overhead is invisible.</p><p>Bearing instrumentation noise in mind, it&apos;s wise to assume
that the times listed for the fastest, shortest subroutines are
going to have a lot of froth and air in them.  They can still be
eating a lot of time if they are called very frequently, however, 
so pay particular attention to their call-count statistics.</p><p>The external-latency problem is also fundamental.  There are 
various sorts of delay and distortion that can happen behind the 
profiler&apos;s back. The simplest is overhead from operations with
unpredictable latency &#x2014; disk and network accesses, cache fills,
process-context switches, and the like.  The problem is not so much
that these overheads happen &#x2014; they may actually be what you&apos;re
trying to measure, especially if you&apos;re focusing on whole-system
performance rather than just tuning a critical inner loop.  The
problem is that they have a random component that means the
results from any individual profiling run may not be very useful.</p><p>One way to minimize the effects of these noise sources, and get a
better picture of where the time is going in the average case, is to
add together the results from a lot of profiling runs.  There are a lot
of good reasons to build test harnesses and test loads for your
programs before you get to optimizing; the most important reason, usually
far more important than performance tuning, is so you can
regression-test your program for correctness as you change it.  Once
you&apos;ve done this, being able to profile repeated tests under load is a
nice side effect that will often give you better information than
a few runs by hand.</p><p>Various effects tend to allocate time spent to calling routines
rather than callees, overweighting upper modes in the call graph.
Function-call overhead, for example, is often charged to the calling
routine (whether or not this is true depends partly on your machine
architecture and where the profiler is allowed to insert probes).
Macros and inline functions, if your compiler supports them, won&apos;t
show up in the profiling report at all; every bit of their time gets
charged to the calling function.</p><p>More importantly, many time-reporting tools give a display in
which time spent in subroutines is charged to the caller. (The
gprof(1)
profiler distributed with open-source Unixes has this trait.)
Na&#xFFFD;vely subtracting callee time from caller time won&apos;t give you a
useful result if the same routine can have more than one caller
&#x2014; the effect would be to artificially deflate both callers&apos;
times. Especially nasty is the common case of a utility function with
multiple call sites, some of which make lots of trivial calls and
others of which make a few complicated ones.</p><p>To get more transparent results, factor your code so that
upper-level routines consist as much as possible of calls to
lower-level routines, rather than in-line code.  If you keep
the overhead of upper-level control logic to a minimum, the
call structure of the code will tend to organize the profile report
in a way that is relatively easy to read.</p><p>You&apos;ll get more insight from using profilers if you think of them less
as ways to collect individual performance numbers, and more as ways to
learn how performance varies as a function of interesting parameters
(e.g., problem size, CPU speed, disc speed, memory size, compiler
optimization, or whatever else is relevant).  Try fitting those
numbers to a model, using open-source software like R or a good-quality
proprietary tool like MATLAB.</p><div class="blockquote"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top">&#xFFFD;</td><td width="80%" valign="top"><p>The natural smoothing of the data that results from model
fitting tends to focus on the big effects and cover up the small,
noisy ones.  For example, by fitting a cubic to the matrix inversion
routine in MATLAB on random matrices from 10 &#xFFFD; 10 to 1000 &#xFFFD; 1000, it is
clear that we actually have three cubics, with clearly defined
boundaries, that correspond roughly to &#x201C;in cache&#x201D;,
&#x201C;in memory but out of cache&#x201D;, and &#x201C;out of
memory&#x201D;.  The data shows us this effect even if weren&apos;t looking
for it, just by looking at the deviations from the best fit.</p></td><td width="10%" valign="top">&#xFFFD;</td></tr><tr><td colspan="2" align="right" valign="top">--<span class="attribution">
<span class="author">Steve Johnson</span>
</span></td><td width="10%" valign="top">&#xFFFD;</td></tr></table></div></div>
</body>
</html>